from MyTokenizer import MyTokenizer
from HFTokenizer import HFTokenizer
from SaGe_main.src.sage_tokenizer import *
import pickle


class MySageTokenizer(MyTokenizer):
    
    def __init__(self, language, training_corpus_dir, vocab_size, algo_name, embedding_schedule, full_vocab_schedule):
        self.language = language
        self.training_corpus_dir = training_corpus_dir
        self.vocab_size = vocab_size
        self.algo_name = algo_name
        self.embedding_schedule = embedding_schedule
        self.full_vocab_schedule = full_vocab_schedule
        self.experiment_name = f"{self.language}_{self.algo_name}_{vocab_size}"
        self.initial_hexed_vocab_path = f"./results/{self.experiment_name}/initial_vocab.vocab"
        self.tokenizer = None
        self.hf_tokenizer = None
        # Updated to be {vocab_size: pruned_byte_tokens}
        self.pruned_tokens = dict()
    
    def __repr__(self):
        return f"{self.language}_{self.vocab_size}_{self.algo_name}"

    def tokenize(self, text):
        return self.tokenizer.tokenize_to_encoded_str(text)
    
    def train_tokenizer(self):
        # BPE or UNI algo
        vocab_builder_algo = self.algo_name.split("_")[0]
        # Train a BPE or UNI tokenizer to create initial vocabulary
        self.hf_tokenizer = HFTokenizer(self.language, self.training_corpus_dir, self.full_vocab_schedule[0], vocab_builder_algo)
        self.hf_tokenizer.train_tokenizer()
        vocab = sorted(list(self.hf_tokenizer.tokenizer.get_vocab().keys()))
        # Turn the vocabulary from letters to hexadecimal format, and add certain tokens that might be missing
        hexed_vocab = self._add_single_bytes(self._hex_vocab(vocab))
        max_len = max([len(bytes.fromhex(str(v))) for v in hexed_vocab])
        
        # Save the hexed vocabulary to a .vocab file
        with open(self.initial_hexed_vocab_path, 'w', encoding='utf-8') as vocab_file:
            for hexed_v in hexed_vocab:
                vocab_file.write(f"{hexed_v}\n")
        
        # Build the SaGe vocabulary
        trainer = SaGeVocabBuilder(full_vocab_schedule=self.full_vocab_schedule,
                                   embeddings_schedule=self.embedding_schedule,
                                   workers_number=4, max_len=max_len)
        trainer.build_vocab(experiment_name=self.experiment_name, corpus_filepath=self.training_corpus_dir,pruned_tokens=self.pruned_tokens,
                            vocabulary_filepath=self.initial_hexed_vocab_path)
        self.save_pruned_tokens(f"./results/{self.experiment_name}/pruned_tokens.txt")
        # The final SaGe vocab is saved to a .vocab file in a certain path. Opens the file and turns it to bytes format for SaGeTokenizer object
        with open(self._get_final_vocab_path(), "r") as f:
            initial_vocab = [bytes.fromhex(line.strip()) for line in f]
        tokenizer = SaGeTokenizer(initial_vocabulary=initial_vocab)
        
        self.tokenizer = tokenizer
    
    
    def _hex_vocab(self, vocab):    
        """
        Translates the SaGE vocabulary to hexadecimal format
        :param vocab: list of vocabulary words generated by BPE or UNI or other tokenizers
        :return: list of hexadecimal vocabulary
        """
        hexed_vocab = []
        for v in vocab:
            hex_token = v.encode("utf-8").hex()
            hexed_vocab.append(hex_token)
        return hexed_vocab
    
    def _add_single_bytes(self, vocab):
        """
        SaGe requires all single bytes to be in the vocabulary. This function adds them in to the vocabulary in hexadecimal
        format, if needed
        :param vocab: list of hexadecimal vocabulary
        :return: updated vocabulary
        """
        for i in range(256):
            t = f"{i:02x}"
            if t not in vocab:
                vocab.append(t)
        return vocab
    
    def save_pruned_tokens(self, path):
        
        with open(path, "w") as f:
            for cur_vocab_size, byte_tokens_list in self.pruned_tokens.items():
                f.write(f"Tokens pruned at vocab size {cur_vocab_size}\n")
                for byte_token in byte_tokens_list:
                    t = byte_token.decode("utf-8", errors="replace")
                    f.write(f"{t}\n")
        
        
    def _get_final_vocab_path(self):
        return f"./results/{self.experiment_name}/sage_vocabs/active_vocab_{self.vocab_size}.vocab"
    
    def save_tokenizer(self, path):
        with open(f"{path}/{self.__repr__()}.pkl", "wb") as f:
            pickle.dump(self, f)
    
    @classmethod
    def load_tokenizer(self, path):
        with open(path, "rb") as f:
            return pickle.load(f)
    
    def get_algo_name(self):
        return self.algo_name
    
    def get_training_corpus_dir(self):
        return self.training_corpus_dir
    
    def get_vocab_size(self):
        return self.vocab_size
    
    def get_vocab(self):
        # Dict[bytes, int]
        bytes_vocab =  self.tokenizer.get_vocabulary()
        vocab_strings = {token.decode("utf-8", errors="replace"): idx for token, idx in bytes_vocab.items()}
        return vocab_strings.keys()